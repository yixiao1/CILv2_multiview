{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the pretrained ViTs and add two new tokens\n",
    "\n",
    "`[CMD]` and `[SPD]` tokens will be concatenated at the end of the input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Diego\\miniconda3\\envs\\cilv2\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "model = torchvision.models.vit_b_32(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (conv_proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): Sequential(\n",
       "      (encoder_layer_0): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_1): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_2): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_3): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_4): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_5): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_6): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_7): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_8): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_9): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_10): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_11): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (heads): Sequential(\n",
       "    (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_img = torch.rand(1, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = model.image_size\n",
    "patch_size = model.patch_size\n",
    "hidden_dim = model.hidden_dim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical ViT pipeline\n",
    "\n",
    "The following is basically the `forward` method of the `VisionTransformer` class in `torchvision.models`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 768, 7, 7])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv_proj(torch.randn(3, 3, 224, 224)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the image through the convolutional/patch embedding layer\n",
    "# _process_input\n",
    "img = model.conv_proj(random_img)  # [1, 3, 224, 224] => [1, 768, 7, 7]\n",
    "img = img.reshape(1, hidden_dim, (image_size // patch_size) ** 2)  # [1, 768, 7, 7] => [1, 768, 49]\n",
    "img = img.permute(0, 2, 1)  # [1, 768, 49] => [1, 49, 768]\n",
    "\n",
    "# Now the rest of forward\n",
    "# Add the class token\n",
    "cls_token = model.class_token\n",
    "n = img.shape[0]\n",
    "\n",
    "# Expand class token to match batch size\n",
    "cls_token = cls_token.expand(n, -1, -1)  # [1, 1, 768] => [1, 1, 768]\n",
    "img = torch.cat([cls_token, img], dim=1)  # [1, 50, 768]])\n",
    "\n",
    "# Pass the image through the encoder (we implicitly add the pos_embedding here)\n",
    "# TODO: here, add the Command and Speed \"tokens\" (next for ablation study, for now just the mapping)\n",
    "img = img + nn.Linear(1, hidden_dim)(torch.ones(1, requires_grad=True)).view(1, 1, hidden_dim)  # SPEED [1, 50, 768] => [1, 50, 768]\n",
    "img = img + nn.Linear(4, hidden_dim)(torch.ones(4, requires_grad=True)).view(1, 1, hidden_dim)  # COMMAND [1, 50, 768] => [1, 50, 768]\n",
    "img = model.encoder(img)  # [1, 50, 768] => [1, 50, 768]\n",
    "\n",
    "# Pass the result to the head for classification (only the CLS token is used)\n",
    "img = img[:, 0]  # [1, 50, 768] => [1, 768]\n",
    "img = model.heads(img)  # [1, 768] => [1, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.class_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.parameter.Parameter"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.encoder.pos_embedding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if we add `[CMD]` and `[SPD]` tokens to the input sequence, we must interpolate the positional embedding to be of size `[1, 52, 768]`, bascially. We can redefine the `interpolate_embeddings` function from `torchvision.models.vision_transformer` to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function interpolate_embeddings in module torchvision.models.vision_transformer:\n",
      "\n",
      "interpolate_embeddings(image_size: int, patch_size: int, model_state: 'OrderedDict[str, torch.Tensor]', interpolation_mode: str = 'bicubic', reset_heads: bool = False) -> 'OrderedDict[str, torch.Tensor]'\n",
      "    This function helps interpolating positional embeddings during checkpoint loading,\n",
      "    especially when you want to apply a pre-trained model on images with different resolution.\n",
      "    \n",
      "    Args:\n",
      "        image_size (int): Image size of the new model.\n",
      "        patch_size (int): Patch size of the new model.\n",
      "        model_state (OrderedDict[str, torch.Tensor]): State dict of the pre-trained model.\n",
      "        interpolation_mode (str): The algorithm used for upsampling. Default: bicubic.\n",
      "        reset_heads (bool): If true, not copying the state of heads. Default: False.\n",
      "    \n",
      "    Returns:\n",
      "        OrderedDict[str, torch.Tensor]: A state dict which can be loaded into the new model.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torchvision.models.vision_transformer.interpolate_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 768])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state['encoder.pos_embedding'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import OrderedDict\n",
    "import torch.nn as nn\n",
    "\n",
    "def interpolate_embeddings(\n",
    "    image_size: int,\n",
    "    patch_size: int,\n",
    "    model_state: \"OrderedDict[str, torch.Tensor]\",\n",
    "    interpolation_mode: str = \"bicubic\",\n",
    "    reset_heads: bool = False,\n",
    ") -> \"OrderedDict[str, torch.Tensor]\":\n",
    "    \"\"\"This function helps interpolating positional embeddings during checkpoint loading,\n",
    "    especially when you want to apply a pre-trained model on images with different resolution.\n",
    "\n",
    "    Args:\n",
    "        image_size (int): Image size of the new model.\n",
    "        patch_size (int): Patch size of the new model.\n",
    "        model_state (OrderedDict[str, torch.Tensor]): State dict of the pre-trained model.\n",
    "        interpolation_mode (str): The algorithm used for upsampling. Default: bicubic.\n",
    "        reset_heads (bool): If true, not copying the state of heads. Default: False.\n",
    "\n",
    "    Returns:\n",
    "        OrderedDict[str, torch.Tensor]: A state dict which can be loaded into the new model.\n",
    "    \"\"\"\n",
    "    # Shape of pos_embedding is (1, seq_length, hidden_dim)\n",
    "    pos_embedding = model_state[\"encoder.pos_embedding\"]\n",
    "    n, seq_length, hidden_dim = pos_embedding.shape\n",
    "    if n != 1:\n",
    "        raise ValueError(f\"Unexpected position embedding shape: {pos_embedding.shape}\")\n",
    "\n",
    "    new_seq_length = (image_size // patch_size) ** 2 + 1\n",
    "\n",
    "    # Need to interpolate the weights for the position embedding.\n",
    "    # We do this by reshaping the positions embeddings to a 2d grid, performing\n",
    "    # an interpolation in the (h, w) space and then reshaping back to a 1d grid.\n",
    "\n",
    "    # The class token embedding shouldn't be interpolated so we split it up.\n",
    "    seq_length -= 1\n",
    "    new_seq_length -= 1\n",
    "    pos_embedding_token = pos_embedding[:, :1, :]  # [1, 1, 768]\n",
    "    pos_embedding_img = pos_embedding[:, 1:, :]  # [1, seq_length, 768]  ( we have already decreased seq_length by 1)\n",
    "\n",
    "    # (1, seq_length, hidden_dim) -> (1, hidden_dim, seq_length)\n",
    "    pos_embedding_img = pos_embedding_img.permute(0, 2, 1)\n",
    "    seq_length_1d = int(math.sqrt(seq_length))\n",
    "    torch._assert(seq_length_1d * seq_length_1d == seq_length, \"seq_length is not a perfect square!\")\n",
    "\n",
    "    # (1, hidden_dim, seq_length) -> (1, hidden_dim, seq_l_1d, seq_l_1d)\n",
    "    pos_embedding_img = pos_embedding_img.reshape(1, hidden_dim, seq_length_1d, seq_length_1d)\n",
    "    new_seq_length_1d = image_size // patch_size\n",
    "\n",
    "    # Perform interpolation.\n",
    "    # (1, hidden_dim, seq_l_1d, seq_l_1d) -> (1, hidden_dim, new_seq_l_1d, new_seq_l_1d)\n",
    "    new_pos_embedding_img = nn.functional.interpolate(\n",
    "        pos_embedding_img,\n",
    "        size=new_seq_length_1d,\n",
    "        mode=interpolation_mode,\n",
    "        align_corners=True,\n",
    "    )\n",
    "\n",
    "    # (1, hidden_dim, new_seq_l_1d, new_seq_l_1d) -> (1, hidden_dim, new_seq_length)\n",
    "    new_pos_embedding_img = new_pos_embedding_img.reshape(1, hidden_dim, new_seq_length)\n",
    "\n",
    "    # (1, hidden_dim, new_seq_length) -> (1, new_seq_length, hidden_dim)\n",
    "    new_pos_embedding_img = new_pos_embedding_img.permute(0, 2, 1)\n",
    "    new_pos_embedding = torch.cat([pos_embedding_token, new_pos_embedding_img], dim=1)\n",
    "\n",
    "    model_state[\"encoder.pos_embedding\"] = new_pos_embedding\n",
    "\n",
    "    if reset_heads:\n",
    "        model_state_copy: \"OrderedDict[str, torch.Tensor]\" = OrderedDict()\n",
    "        for k, v in model_state.items():\n",
    "            if not k.startswith(\"heads\"):\n",
    "                model_state_copy[k] = v\n",
    "        model_state = model_state_copy\n",
    "\n",
    "    return model_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_pos_embedding(model: nn.Module, new_pos_embed_seq_len: int) -> 'OrderedDict[str, torch.Tensor]':\n",
    "    \"\"\" Interpolate position encoding to the new sequence length. \"\"\"\n",
    "    old_state_dict = model.state_dict()\n",
    "\n",
    "    # Get the position embedding from the old model\n",
    "    pos_embed = old_state_dict[\"encoder.pos_embedding\"]\n",
    "    n, old_seq_len, d = pos_embed.shape\n",
    "    pass\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the attention maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderBlock(\n",
       "  (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (self_attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (mlp): MLPBlock(\n",
       "    (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (act): GELU()\n",
       "    (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "    (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(model.encoder.layers, f'encoder_layer_{0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.feature_extraction import get_graph_node_names, create_feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoder.layers.encoder_layer_11.dim': 3}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_feature_extractor(model, ['encoder.layers.encoder_layer_11.dim'])(random_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_11 = [x for x in get_graph_node_names(model)[0] if 'encoder_layer_11' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['encoder.layers.encoder_layer_11.dim',\n",
       " 'encoder.layers.encoder_layer_11.eq',\n",
       " 'encoder.layers.encoder_layer_11.getattr',\n",
       " 'encoder.layers.encoder_layer_11._assert',\n",
       " 'encoder.layers.encoder_layer_11.ln',\n",
       " 'encoder.layers.encoder_layer_11.self_attention',\n",
       " 'encoder.layers.encoder_layer_11.getitem',\n",
       " 'encoder.layers.encoder_layer_11.getitem_1',\n",
       " 'encoder.layers.encoder_layer_11.dropout',\n",
       " 'encoder.layers.encoder_layer_11.add',\n",
       " 'encoder.layers.encoder_layer_11.ln_1',\n",
       " 'encoder.layers.encoder_layer_11.mlp.linear',\n",
       " 'encoder.layers.encoder_layer_11.mlp.act',\n",
       " 'encoder.layers.encoder_layer_11.mlp.dropout',\n",
       " 'encoder.layers.encoder_layer_11.mlp.linear_1',\n",
       " 'encoder.layers.encoder_layer_11.mlp.dropout_1',\n",
       " 'encoder.layers.encoder_layer_11.add_1']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 768])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor(random_img)['encoder.layers.encoder_layer_11.self_attention'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.layers.encoder_layer_11.dim\n",
      "3\n",
      "encoder.layers.encoder_layer_11.eq\n",
      "True\n",
      "encoder.layers.encoder_layer_11.getattr\n",
      "1\n",
      "encoder.layers.encoder_layer_11._assert\n",
      "None\n",
      "encoder.layers.encoder_layer_11.ln\n",
      "torch.Size([1, 50, 768])\n",
      "encoder.layers.encoder_layer_11.self_attention\n",
      "torch.Size([1, 50, 768])\n",
      "encoder.layers.encoder_layer_11.getitem\n",
      "torch.Size([1, 50, 768])\n",
      "encoder.layers.encoder_layer_11.getitem_1\n",
      "None\n",
      "encoder.layers.encoder_layer_11.dropout\n",
      "torch.Size([1, 50, 768])\n",
      "encoder.layers.encoder_layer_11.add\n",
      "torch.Size([1, 50, 768])\n",
      "encoder.layers.encoder_layer_11.ln_1\n",
      "torch.Size([1, 50, 768])\n",
      "encoder.layers.encoder_layer_11.mlp.linear\n",
      "torch.Size([1, 50, 3072])\n",
      "encoder.layers.encoder_layer_11.mlp.act\n",
      "torch.Size([1, 50, 3072])\n",
      "encoder.layers.encoder_layer_11.mlp.dropout\n",
      "torch.Size([1, 50, 3072])\n",
      "encoder.layers.encoder_layer_11.mlp.linear_1\n",
      "torch.Size([1, 50, 768])\n",
      "encoder.layers.encoder_layer_11.mlp.dropout_1\n",
      "torch.Size([1, 50, 768])\n",
      "encoder.layers.encoder_layer_11.add_1\n",
      "torch.Size([1, 50, 768])\n"
     ]
    }
   ],
   "source": [
    "# With get_graph_node_names, get all the node names in layer 11\n",
    "# get_graph_node_names(getattr(model.encoder.layers, f'encoder_layer_{11}'))[0]\n",
    "\n",
    "# feature_extractor(random_img)['encoder.layers.encoder_layer_11.self_attention'][0].shape\n",
    "\n",
    "# jk = [name for name, _ in getattr(model.encoder.layers, f'encoder_layer_{11}').named_children()]\n",
    "\n",
    "for whatevs in layer_11:\n",
    "    fe = create_feature_extractor(model, [whatevs])\n",
    "    print(whatevs)\n",
    "    if isinstance(fe(random_img)[whatevs], torch.Tensor):\n",
    "        print(fe(random_img)[whatevs].shape)\n",
    "    elif isinstance(fe(random_img)[whatevs], (list, tuple)):\n",
    "        temp = fe(random_img)[whatevs][0]\n",
    "        if isinstance(temp, torch.Tensor):\n",
    "            print(temp.shape)\n",
    "        else:\n",
    "            print(temp)\n",
    "    else:\n",
    "        print(fe(random_img)[whatevs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LayerNorm((768,), eps=1e-06, elementwise_affine=True)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.layers.encoder_layer_11.ln_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "# Just for the first layer\n",
    "n, c, h, w = random_img.shape\n",
    "patch_size = model.patch_size\n",
    "\n",
    "n_h = h // patch_size\n",
    "n_w = w // patch_size\n",
    "\n",
    "pos_embed = model.encoder.pos_embedding  # [1, 50, 768]\n",
    "\n",
    "x = model.conv_proj(random_img)  # [1, 768, 7, 7]\n",
    "x = x.reshape(n, model.hidden_dim, n_h * n_w).permute(0, 2, 1)  # [1, 49, 768]\n",
    "\n",
    "batch_class_token = model.class_token.expand(n, -1, -1)  # [1, 1, 768]\n",
    "x = torch.cat((batch_class_token, x), dim=1)  # [1, 50, 768]\n",
    "x = x + pos_embed  # [1, 50, 768]\n",
    "\n",
    "x = model.encoder(x)  # [1, 50, 768]\n",
    "\n",
    "x = x[:, 0]  # [1, 768]\n",
    "\n",
    "x = model.heads(x)  # [1, 1000]\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 torch.Size([1, 50, 768])\n",
      "2 torch.Size([1, 50, 768]) torch.Size([1, 50, 50])\n",
      "3 torch.Size([1, 50, 768])\n",
      "4 torch.Size([1, 50, 768])\n",
      "5 torch.Size([1, 50, 768])\n",
      "6 torch.Size([1, 50, 768])\n",
      "7 torch.Size([1, 50, 768])\n"
     ]
    }
   ],
   "source": [
    "# Just for the first layer\n",
    "n, c, h, w = random_img.shape\n",
    "patch_size = model.patch_size\n",
    "\n",
    "n_h = h // patch_size\n",
    "n_w = w // patch_size\n",
    "\n",
    "x = model.conv_proj(random_img)  # [1, 768, 7, 7]\n",
    "x = x.reshape(n, model.hidden_dim, n_h * n_w).permute(0, 2, 1)  # [1, 49, 768]\n",
    "\n",
    "batch_class_token = model.class_token.expand(n, -1, -1)  # [1, 1, 768]\n",
    "x = torch.cat((batch_class_token, x), dim=1)  # [1, 50, 768]\n",
    "x = x + model.encoder.pos_embedding  # [1, 50, 768]\n",
    "\n",
    "layer0 = getattr(model.encoder.layers, f'encoder_layer_{0}')\n",
    "\n",
    "x0 = layer0.ln_1(x)\n",
    "print(1, x0.shape)\n",
    "x0, a = layer0.self_attention(x0, x0, x0, need_weights=True)  # THIS IS IT MOTHELDJFSFD\n",
    "print(2, x0.shape, a.shape)\n",
    "x0 = layer0.dropout(x0)\n",
    "print(3, x0.shape)\n",
    "x0 = x0 + x\n",
    "print(4, x0.shape)\n",
    "x1 = layer0.ln_2(x0)\n",
    "print(5, x1.shape)\n",
    "x1 = layer0.mlp(x1)\n",
    "print(6, x1.shape)\n",
    "x2 = x0 + x1\n",
    "print(7, x2.shape)\n",
    "# x = layer0(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 768])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the attention map of a specific layer of a Vision Transformer\n",
    "# Define a forward wrapper to extract the attention map\n",
    "\n",
    "class AttentionMapExtractor(nn.Module):\n",
    "    def __init__(self, model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def get_attention_map_layer(self, layer: int):\n",
    "        return self.model.encoder.layers[layer].attn.attn[0].cpu().detach().numpy()\n",
    "    \n",
    "    def get_attention_map_head(self, layer: int, head: int):\n",
    "        return self.model.encoder.layers[layer].attn.attn[0][head].cpu().detach().numpy()\n",
    "    \n",
    "# Usage\n",
    "model = AttentionMapExtractor(model)\n",
    "model.eval()\n",
    "\n",
    "# Extract the attention map of a specific layer of a Vision Transformer\n",
    "\n",
    "def extract_attention_map(model: nn.Module, \n",
    "                          layer: int):\n",
    "    \"\"\" Extract the attention map of a specific layer of a Vision Transformer \"\"\"\n",
    "    encoder = model.encoder\n",
    "    num_layers = len(encoder.layers)\n",
    "    layer = max(0, min(layer, num_layers - 1))  # Sanity check\n",
    "    # Get the specific layer\n",
    "    layer = getattr(model.encoder.layers, f'encoder_layer_{layer}')\n",
    "    # Get the attention map of a specific layer\n",
    "    attn_map = model.encoder.layers[layer].attn.attn[0].cpu().detach().numpy()  # [1, 12, 50, 50]\n",
    "\n",
    "    # Get the attention map of the first head\n",
    "    attn_map = attn_map[0]  # [50, 50]\n",
    "\n",
    "    # Plot the attention map\n",
    "    plt.imshow(attn_map)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cilv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
