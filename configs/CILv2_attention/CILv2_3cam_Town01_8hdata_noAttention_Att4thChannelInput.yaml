#### Model Related Parameters ####
#### Training Related Parameters ####
MAGICAL_SEED: 1314520
DATA_PARALLEL: True
BATCH_SIZE: 512
NUM_WORKER: 5
NUMBER_EPOCH: 80
TARGETS: ['steer', 'acceleration']  # the targets that the network should estimate
ACCELERATION_AS_ACTION: True
OTHER_INPUTS: ['speed', 'direction'] # extra input to the neural network
TRAIN_DATASET_NAME: ['Roach_carla0913_fps10_640x640_3cam_attention/training_Town01_dense_clearnoon',
                    'Roach_carla0913_fps10_640x640_3cam_attention/training_Town01_dense_clearsunset',
                    'Roach_carla0913_fps10_640x640_3cam_attention/training_Town01_dense_hardrainnoon',
                    'Roach_carla0913_fps10_640x640_3cam_attention/training_Town01_dense_wetnoon'] # Folders of the used training datasets. Should be inside DATASET_PATH folder
VALID_DATASET_NAME: ['valid/valid_Town02_busy_clearnoon'] # Folders of the used offline evaluation datasets. Should be inside DATASET_PATH folder

ENCODER_INPUT_FRAMES_NUM: 1
ENCODER_STEP_INTERVAL: 1
ENCODER_OUTPUT_STEP_DELAY: 0
DECODER_OUTPUT_FRAMES_NUM: 1
IMG_NORMALIZATION:     # ImageNet normalization
  mean: [0.485, 0.456, 0.406]
  std: [0.229, 0.224, 0.225]
IMAGE_SHAPE: [3, 300, 300]  # Input image shape
DATA_USED: ['resized_rgb_left', 'resized_rgb_central', 'resized_rgb_right',  # Multi-view cameras, it needs to be set in this order
            'virtual_attention_left_', 'virtual_attention_central_', 'virtual_attention_right_']  # Virtual attention maps per camera
ATTENTION_AS_INPUT: True

DATA_COMMAND_ONE_HOT: True   # encode high-level command to one-hot
DATA_COMMAND_CLASS_NUM: 4    # 4 for single-lane small towns, 6 for multi-lane towns (lane change to right/left)
# Data normalization: might be changed depending on datasets
DATA_NORMALIZATION:
  steer: [-1.0, 1.0]
  acceleration: [-1.0, 1.0]
  speed: [-1.0, 11.0]     # m/s
VIRTUAL_ATTENTION_INTERPOLATION: 'INTER_AREA'  # 'INTER_LINEAR' (bilinear), 'INTER_AREA', 'INTER_NEAREST'
BINARIZE_ATTENTION: False

# Loss Parameters #
LOSS: 'Action_nospeed_L1'  # no speed L1 loss
ATTENTION_LOSS: False
LOSS_WEIGHT:
  actions:
    steer: 0.5
    acceleration: 0.5

# Optimizer Parameters #
LEARNING_RATE: 0.0001
LEARNING_RATE_MINIMUM: 0.00001
LEARNING_RATE_DECAY_EPOCHES: [30, 50, 65]
LEARNING_RATE_SCHEDULE: 'step'
LEARNING_RATE_POLICY:
  name: 'normal'
  level: 0.5

#### Validation Related Parameters ####
EVAL_SAVE_LAST_Conv_ACTIVATIONS: True
EVAL_BATCH_SIZE: 30
EVAL_SAVE_EPOCHES: [1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80]
EARLY_STOPPING: False
EVAL_IMAGE_WRITING_NUMBER: 100
EVAL_DRAW_OFFLINE_RESULTS_GRAPHS: ['MAE_steer', 'MAE_acceleration', 'MAE']

### Network Parameters ####
# Encoder part#
IMAGENET_PRE_TRAINED: True
NO_ACT_TOKENS: True  # Remove all [ACT] tokens
EARLY_ATTENTION: False

MODEL_TYPE: 'CIL_multiview'
# Based on the MODEL_TYPE, we specify the structure
MODEL_CONFIGURATION:
  encoder_embedding:
    perception:
      res:
        name: 'resnet34'
        layer_id: 4

  TxEncoder:
    d_model: 512
    n_head: 4
    num_layers: 4
    norm_first: True
    learnable_pe: True

  command:
    fc:
      neurons: [512]
      dropouts: [0.0]

  speed:
    fc:
      neurons: [512]
      dropouts: [0.0]

  action_output:
    fc:
      neurons: [512, 256]
      dropouts: [0.0, 0.0]