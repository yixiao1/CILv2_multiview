{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the pretrained ViTs and add two new tokens\n",
    "\n",
    "`[CMD]` and `[SPD]` tokens will be concatenated at the end of the input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dporres/miniconda3/envs/cilv2/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "model = torchvision.models.vit_b_32(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (conv_proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): Sequential(\n",
       "      (encoder_layer_0): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_1): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_2): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_3): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_4): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_5): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_6): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_7): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_8): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_9): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_10): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_11): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (heads): Sequential(\n",
       "    (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_img = torch.rand(1, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = model.image_size\n",
    "patch_size = model.patch_size\n",
    "hidden_dim = model.hidden_dim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical ViT pipeline\n",
    "\n",
    "The following is basically the `forward` method of the `VisionTransformer` class in `torchvision.models`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 768, 7, 7])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv_proj(torch.randn(3, 3, 224, 224)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the image through the convolutional/patch embedding layer\n",
    "# _process_input\n",
    "img = model.conv_proj(random_img)  # [1, 3, 224, 224] => [1, 768, 7, 7]\n",
    "img = img.reshape(1, hidden_dim, (image_size // patch_size) ** 2)  # [1, 768, 7, 7] => [1, 768, 49]\n",
    "img = img.permute(0, 2, 1)  # [1, 768, 49] => [1, 49, 768]\n",
    "\n",
    "# Now the rest of forward\n",
    "# Add the class token\n",
    "cls_token = model.class_token\n",
    "n = img.shape[0]\n",
    "\n",
    "# Expand class token to match batch size\n",
    "cls_token = cls_token.expand(n, -1, -1)  # [1, 1, 768] => [1, 1, 768]\n",
    "img = torch.cat([cls_token, img], dim=1)  # [1, 50, 768]])\n",
    "\n",
    "# Pass the image through the encoder (we implicitly add the pos_embedding here)\n",
    "# TODO: here, add the Command and Speed \"tokens\" (next for ablation study, for now just the mapping)\n",
    "img = img + nn.Linear(1, hidden_dim)(torch.ones(1, requires_grad=True)).view(1, 1, hidden_dim)  # SPEED [1, 50, 768] => [1, 50, 768]\n",
    "img = img + nn.Linear(4, hidden_dim)(torch.ones(4, requires_grad=True)).view(1, 1, hidden_dim)  # COMMAND [1, 50, 768] => [1, 50, 768]\n",
    "img = model.encoder(img)  # [1, 50, 768] => [1, 50, 768]\n",
    "\n",
    "# Pass the result to the head for classification (only the CLS token is used)\n",
    "img = img[:, 0]  # [1, 50, 768] => [1, 768]\n",
    "img = model.heads(img)  # [1, 768] => [1, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.class_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.parameter.Parameter"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.encoder.pos_embedding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if we add `[CMD]` and `[SPD]` tokens to the input sequence, we must interpolate the positional embedding to be of size `[1, 52, 768]`, bascially. We can redefine the `interpolate_embeddings` function from `torchvision.models.vision_transformer` to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function interpolate_embeddings in module torchvision.models.vision_transformer:\n",
      "\n",
      "interpolate_embeddings(image_size: int, patch_size: int, model_state: 'OrderedDict[str, torch.Tensor]', interpolation_mode: str = 'bicubic', reset_heads: bool = False) -> 'OrderedDict[str, torch.Tensor]'\n",
      "    This function helps interpolating positional embeddings during checkpoint loading,\n",
      "    especially when you want to apply a pre-trained model on images with different resolution.\n",
      "    \n",
      "    Args:\n",
      "        image_size (int): Image size of the new model.\n",
      "        patch_size (int): Patch size of the new model.\n",
      "        model_state (OrderedDict[str, torch.Tensor]): State dict of the pre-trained model.\n",
      "        interpolation_mode (str): The algorithm used for upsampling. Default: bicubic.\n",
      "        reset_heads (bool): If true, not copying the state of heads. Default: False.\n",
      "    \n",
      "    Returns:\n",
      "        OrderedDict[str, torch.Tensor]: A state dict which can be loaded into the new model.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torchvision.models.vision_transformer.interpolate_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 768])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state['encoder.pos_embedding'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import OrderedDict\n",
    "import torch.nn as nn\n",
    "\n",
    "def interpolate_embeddings(\n",
    "    image_size: int,\n",
    "    patch_size: int,\n",
    "    model_state: \"OrderedDict[str, torch.Tensor]\",\n",
    "    interpolation_mode: str = \"bicubic\",\n",
    "    reset_heads: bool = False,\n",
    ") -> \"OrderedDict[str, torch.Tensor]\":\n",
    "    \"\"\"This function helps interpolating positional embeddings during checkpoint loading,\n",
    "    especially when you want to apply a pre-trained model on images with different resolution.\n",
    "\n",
    "    Args:\n",
    "        image_size (int): Image size of the new model.\n",
    "        patch_size (int): Patch size of the new model.\n",
    "        model_state (OrderedDict[str, torch.Tensor]): State dict of the pre-trained model.\n",
    "        interpolation_mode (str): The algorithm used for upsampling. Default: bicubic.\n",
    "        reset_heads (bool): If true, not copying the state of heads. Default: False.\n",
    "\n",
    "    Returns:\n",
    "        OrderedDict[str, torch.Tensor]: A state dict which can be loaded into the new model.\n",
    "    \"\"\"\n",
    "    # Shape of pos_embedding is (1, seq_length, hidden_dim)\n",
    "    pos_embedding = model_state[\"encoder.pos_embedding\"]\n",
    "    n, seq_length, hidden_dim = pos_embedding.shape\n",
    "    if n != 1:\n",
    "        raise ValueError(f\"Unexpected position embedding shape: {pos_embedding.shape}\")\n",
    "\n",
    "    new_seq_length = (image_size // patch_size) ** 2 + 1\n",
    "\n",
    "    # Need to interpolate the weights for the position embedding.\n",
    "    # We do this by reshaping the positions embeddings to a 2d grid, performing\n",
    "    # an interpolation in the (h, w) space and then reshaping back to a 1d grid.\n",
    "\n",
    "    # The class token embedding shouldn't be interpolated so we split it up.\n",
    "    seq_length -= 1\n",
    "    new_seq_length -= 1\n",
    "    pos_embedding_token = pos_embedding[:, :1, :]  # [1, 1, 768]\n",
    "    pos_embedding_img = pos_embedding[:, 1:, :]  # [1, seq_length, 768]  ( we have already decreased seq_length by 1)\n",
    "\n",
    "    # (1, seq_length, hidden_dim) -> (1, hidden_dim, seq_length)\n",
    "    pos_embedding_img = pos_embedding_img.permute(0, 2, 1)\n",
    "    seq_length_1d = int(math.sqrt(seq_length))\n",
    "    torch._assert(seq_length_1d * seq_length_1d == seq_length, \"seq_length is not a perfect square!\")\n",
    "\n",
    "    # (1, hidden_dim, seq_length) -> (1, hidden_dim, seq_l_1d, seq_l_1d)\n",
    "    pos_embedding_img = pos_embedding_img.reshape(1, hidden_dim, seq_length_1d, seq_length_1d)\n",
    "    new_seq_length_1d = image_size // patch_size\n",
    "\n",
    "    # Perform interpolation.\n",
    "    # (1, hidden_dim, seq_l_1d, seq_l_1d) -> (1, hidden_dim, new_seq_l_1d, new_seq_l_1d)\n",
    "    new_pos_embedding_img = nn.functional.interpolate(\n",
    "        pos_embedding_img,\n",
    "        size=new_seq_length_1d,\n",
    "        mode=interpolation_mode,\n",
    "        align_corners=True,\n",
    "    )\n",
    "\n",
    "    # (1, hidden_dim, new_seq_l_1d, new_seq_l_1d) -> (1, hidden_dim, new_seq_length)\n",
    "    new_pos_embedding_img = new_pos_embedding_img.reshape(1, hidden_dim, new_seq_length)\n",
    "\n",
    "    # (1, hidden_dim, new_seq_length) -> (1, new_seq_length, hidden_dim)\n",
    "    new_pos_embedding_img = new_pos_embedding_img.permute(0, 2, 1)\n",
    "    new_pos_embedding = torch.cat([pos_embedding_token, new_pos_embedding_img], dim=1)\n",
    "\n",
    "    model_state[\"encoder.pos_embedding\"] = new_pos_embedding\n",
    "\n",
    "    if reset_heads:\n",
    "        model_state_copy: \"OrderedDict[str, torch.Tensor]\" = OrderedDict()\n",
    "        for k, v in model_state.items():\n",
    "            if not k.startswith(\"heads\"):\n",
    "                model_state_copy[k] = v\n",
    "        model_state = model_state_copy\n",
    "\n",
    "    return model_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_pos_embedding(model: nn.Module, new_pos_embed_seq_len: int) -> 'OrderedDict[str, torch.Tensor]':\n",
    "    \"\"\" Interpolate position encoding to the new sequence length. \"\"\"\n",
    "    old_state_dict = model.state_dict()\n",
    "\n",
    "    # Get the position embedding from the old model\n",
    "    pos_embed = old_state_dict[\"encoder.pos_embedding\"]\n",
    "    n, old_seq_len, d = pos_embed.shape\n",
    "    pass\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cilv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8088119c7d02d44ca49a146b9580d8a1bd4938fa72d8bf0539c55214160763d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
